{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frYrbQjRFJxO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def make_dataset(dir):\n",
    "    # f1 = \"working_60k/reranked.json\"\n",
    "    f1 = \"working_6k_sci/reranked.json\"\n",
    "    f2 = \"working_6k_nonsci/reranked.json\"\n",
    "    f3 = \"working/reranked.json\"\n",
    "\n",
    "    columns = ['prompt', 'A', 'C', 'B', 'D', 'E', 'answer', 'tier_2_passages']\n",
    "    \n",
    "    train = [f1, f2]\n",
    "    validation = [f3]\n",
    "    train_dfs = [pd.DataFrame.from_records(json.load(open(f))) for f in train]\n",
    "    train_df = pd.concat(train_dfs)\n",
    "    train_df = train_df[columns]\n",
    "    train_df.to_csv(f\"{dir}/train.csv\", index=False)\n",
    "\n",
    "    val_dfs = [pd.DataFrame.from_records(json.load(open(f))) for f in validation]\n",
    "    val_df = pd.concat(val_dfs)\n",
    "    val_df = val_df[columns]\n",
    "    # val_df['wikipedia_excerpt'] = None\n",
    "    val_df.to_csv(f\"{dir}/validation.csv\", index=False)\n",
    "\n",
    "make_dataset(\"kaggle_sci_qa/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frYrbQjRFJxO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "swag = load_dataset(\"kaggle_sci_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frYrbQjRFJxO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frYrbQjRFJxO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = \"sileod/deberta-v3-large-tasksource-nli\"\n",
    "# model_path = \"deberta_ft/checkpoint-3005/\"\n",
    "# model_path = \"microsoft/deberta-v3-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QbjBe_IRFJxP",
    "outputId": "1f6f0fc1-98ac-439b-f8e4-90b57276c9e7"
   },
   "outputs": [],
   "source": [
    "swag[\"train\"][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_TPr2x_FJxQ"
   },
   "source": [
    "While it looks like there are a lot of fields here, it is actually pretty straightforward:\n",
    "\n",
    "- `sent1` and `sent2`: these fields show how a sentence starts, and if you put the two together, you get the `startphrase` field.\n",
    "- `ending`: suggests a possible ending for how a sentence can end, but only one of them is correct.\n",
    "- `label`: identifies the correct sentence ending."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xsSd9B7FJxQ"
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2eqcumMFJxQ"
   },
   "source": [
    "The next step is to load a BERT tokenizer to process the sentence starts and the four possible endings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uRDFZcVFJxQ"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uRDFZcVFJxQ"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_path, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E405KK4wFJxQ"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "ABCDE = \"ABCDE\"\n",
    "ending_names = [str(i) for i in range(5)]\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    first_sentences = [[context] * 5 for context in examples[\"context\"]]\n",
    "    \n",
    "    question_headers = examples[\"prompt\"]\n",
    "    second_sentences = [\n",
    "        [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\n",
    "    ]\n",
    "\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    # print(first_sentences, second_sentences)\n",
    "    \n",
    "    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "    return {k: [v[i : i + 5] for i in range(0, len(v), 5)] for k, v in tokenized_examples.items()}\n",
    "\n",
    "# def keep_only_context(example):\n",
    "#     return len(example['support']) > 0\n",
    "\n",
    "def make_labels(example):\n",
    "    answer = example['answer']\n",
    "    example['label'] = ABCDE.index(answer)\n",
    "    for i in range(5):\n",
    "        example[str(i)] = example[ABCDE[i]]\n",
    "    return example\n",
    "\n",
    "def is_bad_passage(passage):\n",
    "    words = passage.split(\" \")\n",
    "    lines = passage.split(\"\\n\")\n",
    "    n_words_per_line = len(words) / len(lines)\n",
    "    isbad = (len(lines) >= 5) and (n_words_per_line <= 5)\n",
    "    if passage.count(\"|\") > 10:\n",
    "        isbad = True\n",
    "    return isbad\n",
    "\n",
    "def make_context(example):\n",
    "    contexts = example[\"tier_2_passages\"]\n",
    "    contexts = eval(contexts)\n",
    "    \n",
    "    # extra_context = example.get('wikipedia_excerpt', '')\n",
    "    # if extra_context:\n",
    "    #     extra_title = extra_context.split(\":\", maxsplit=1)[0]\n",
    "    #     extra_passage = extra_context.split(\":\", maxsplit=1)[1]\n",
    "    \n",
    "    #     extra_context = {'title':extra_title, 'passage':extra_passage}\n",
    "    #     # print(extra_context)\n",
    "    #     contexts = [extra_context] + contexts\n",
    "    #     # contexts = [extra_context]\n",
    "    \n",
    "    openbook = \"\"\n",
    "    max_openbook_len = 1024\n",
    "    for context in contexts:\n",
    "        tokens = tokenizer.encode(openbook)\n",
    "        if len(tokens) > max_openbook_len:\n",
    "            break\n",
    "        passage = context['passage']\n",
    "        if is_bad_passage(passage):\n",
    "            continue\n",
    "        title = context['title']\n",
    "        passage = passage.replace(\"\\n\", \" \")\n",
    "        lpassage = len(tokenizer.encode(passage))\n",
    "        if lpassage > 512:\n",
    "            print(passage)\n",
    "            continue\n",
    "        if lpassage + len(tokens) > max_openbook_len:\n",
    "            continue\n",
    "        openbook += f\"\"\"{title}: {passage}\\n\"\"\"\n",
    "        \n",
    "    \n",
    "    example['context'] = openbook\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnAspdUTFJxQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "swag_context = swag.map(make_context)\n",
    "swag_clean = swag_context.map(make_labels)\n",
    "tokenized_swag = swag_clean.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnAspdUTFJxQ"
   },
   "outputs": [],
   "source": [
    "maxlen = 0\n",
    "badguy = None\n",
    "for elem in tokenized_swag['train']:\n",
    "    for ids in elem['input_ids']:\n",
    "        maxlen = max(len(ids), maxlen)\n",
    "        if len(ids) > 3000:\n",
    "            badguy = elem\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kf0rX1xxFJxR"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbV73sVnFJxR"
   },
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "\n",
    "# accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XuFjXpEeFJxR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def map_at_k(predictions, labels, k):\n",
    "    map_sum = 0\n",
    "    pred = np.argsort(-1*np.array(predictions),axis=1)[:,:k]\n",
    "    for x,y in zip(pred,labels):\n",
    "        z = [1/i if y==j else 0 for i,j in zip(list(range(1,k+1)),x)]\n",
    "        map_sum += np.sum(z)\n",
    "    return map_sum / len(predictions)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # predictions, labels = eval_pred\n",
    "    # predictions = np.argmax(predictions, axis=1)\n",
    "    # acc = accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    predictions = eval_pred.predictions.tolist()\n",
    "    labels = eval_pred.label_ids.tolist()\n",
    "    return {\"map@3\": map_at_k(predictions, labels, 3), \"map@1\": map_at_k(predictions, labels, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVIQFjFzFJxR"
   },
   "outputs": [],
   "source": [
    "requires_grad = False\n",
    "layer_number = 0\n",
    "for name, param in model.deberta.named_parameters():\n",
    "    try:\n",
    "        layer_number = name.split(\".\")[2]\n",
    "        layer_number = int(layer_number)\n",
    "        if layer_number > 23:\n",
    "            requires_grad = True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    param.requires_grad = requires_grad\n",
    "\n",
    "    # print(name, layer_number, \"trainable:\", param.requires_grad, param.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVIQFjFzFJxR"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || \"\n",
    "        f\"all params: {all_param} || \"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYINq7GBFJxR"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"deberta_ft2\",\n",
    "    save_strategy=\"epoch\",\n",
    "    # optim='adamw_bnb_8bit',\n",
    "    # max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    # load_best_model_at_end=True,\n",
    "    # gradient_checkpointing=True,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    logging_steps=50,\n",
    "    eval_steps=100,\n",
    "    evaluation_strategy='steps',\n",
    "    max_steps=61000,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='cosine',\n",
    "    push_to_hub=False,\n",
    "    fp16=True,\n",
    "    tf32=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "tokenized_swag = tokenized_swag.shuffle()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_swag[\"train\"],\n",
    "    eval_dataset=tokenized_swag[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYINq7GBFJxR"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer([\"capital of paris is france. what is capital of france? paris\", \"capital of paris is france. what is capital of france? delhi\"], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = trainer.predict(tokenized_swag[\"validation\"]).predictions\n",
    "predictions_as_ids = np.argsort(-test_predictions, 1)\n",
    "predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n",
    "predictions_as_string = test_df['prediction'] = [\n",
    "    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_as_answer_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv('kaggle_sci_qa/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(swag_clean['validation'][0]['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
